# RAM++ (Recognize Anything Plus Plus) - Image Tagging Model

> **Generated**: 2025-12-21
> **Sources current as of**: December 2025
> **Scope**: Standard
> **Version**: 1.0
> **Audit-Ready**: Yes

---

## Executive Summary / TLDR

RAM++ (Recognize Anything Plus Model) is a state-of-the-art open-set image tagging model developed by researchers at OPPO and Fudan University. It builds on the original RAM model by introducing **multi-grained text supervision** that combines individual tag supervision with global text supervision in a unified alignment framework [1][HIGH].

**Key capabilities:**
- Recognizes **4,585 semantic tag categories** with high accuracy [HIGH]
- Outperforms CLIP by **10.2-15.4 mAP** on predefined categories [1][HIGH]
- Achieves **5.0-6.4 mAP improvement** on open-set categories over CLIP and RAM [1][HIGH]
- Processes images at **~200-300ms per image** on consumer GPUs [4][HIGH]
- Lightweight 3GB model suitable for production deployment [4][HIGH]

RAM++ is particularly effective for automated image tagging, content moderation, visual search, and as a component in larger vision-language pipelines like Grounded-SAM [2].

---

## Background & Context

### The Image Tagging Problem

Traditional image classification assigns a single label to an image. **Image tagging** is more complex: it assigns multiple relevant labels (tags) to describe various aspects of an image - objects, scenes, actions, attributes, and concepts.

Prior approaches fell into two categories:
1. **Supervised tagging models** - Limited to predefined vocabularies, require expensive manual annotation
2. **Vision-language models** (CLIP, BLIP) - Use global image-text alignment but perform sub-optimally on multi-tag recognition [1]

### Evolution: RAM to RAM++

The **Recognize Anything Model (RAM)** was introduced in 2023 as a strong foundation for image tagging, using automatic text semantic parsing to generate annotation-free training data [3]. RAM++ extends this with:

- **Multi-grained text supervision** integrating individual tags with global descriptions
- **LLM-generated tag descriptions** to enrich semantic understanding
- **Improved open-set recognition** for uncommon categories

---

## Model Architecture

### Overview

RAM++ follows an encoder-decoder architecture with three main components [HIGH]:

```
┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐
│  Image Encoder  │────▶│  Tag Embedding  │────▶│  Tagging Head   │
│  (Swin-Large)   │     │   Reweighting   │     │    (BERT)       │
└─────────────────┘     └─────────────────┘     └─────────────────┘
        │                       │                       │
   384x384 RGB            Label Embeddings          Tag Logits
```

### Image Encoder: Swin Transformer Large

RAM++ uses a **Swin Transformer Large** backbone [HIGH]:

| Parameter | Value |
|-----------|-------|
| Input Resolution | 384 x 384 pixels |
| Patch Size | 4 x 4 |
| Embed Dimension | 192 |
| Depths | [2, 2, 18, 2] |
| Attention Heads | [6, 12, 24, 48] |
| Window Size | 12 |
| MLP Ratio | 4.0 |
| Vision Width | 1536 |

The Swin Transformer uses shifted window attention for efficient hierarchical feature extraction, achieving linear computational complexity relative to image size [5].

### Tag Embedding with LLM Descriptions

A key innovation in RAM++ is the use of **51 LLM-generated descriptions per tag** [HIGH]:

1. Each of the 4,585 tags has 51 semantic descriptions generated by an LLM
2. Total embedding size: 4,585 × 51 = 233,835 description embeddings
3. Embeddings are stored in a frozen parameter matrix of shape `[num_class × 51, 512]`
4. Dynamic reweighting selects relevant descriptions based on image content

### Tagging Head: Modified BERT

The tagging head is a **modified BERT model** configured for multi-label classification [HIGH]:

| Component | Configuration |
|-----------|---------------|
| Architecture | BERT (Query2Label style) |
| Hidden Size | 768 |
| Encoder Width | 512 |
| Self-Attention | Removed (for efficiency) |
| Output | Per-tag binary classification |

The self-attention layers are deleted following the Query2Label approach to reduce computation while maintaining performance.

### Loss Functions

RAM++ uses **Asymmetric Loss** for both tagging and text alignment [HIGH]:

```python
# Tagging Loss
AsymmetricLoss(gamma_neg=7, gamma_pos=0, clip=0.05)

# Text Alignment Loss
AsymmetricLoss(gamma_neg=4, gamma_pos=0, clip=0.05)
```

Additionally, L1 loss is used for CLIP feature distillation to align image embeddings with CLIP's representation space.

---

## Tag Vocabulary

### Vocabulary Statistics

| Metric | Value |
|--------|-------|
| Total Tags | 4,585 (after synonymous reduction) |
| Original Tags | ~6,500 (before reduction) |
| Languages | English + Chinese |
| Per-Tag Thresholds | Individually calibrated |

### Tag Categories

The vocabulary covers diverse semantic concepts [MEDIUM]:

- **Objects**: abacus, abalone, accordion, camera, furniture...
- **Scenes**: monastery, academy, beach, forest, city...
- **Actions**: act, activity, running, swimming...
- **Attributes**: 3D, vintage, colorful, abstract...
- **Concepts**: accident, adaptation, celebration...

### Threshold Calibration

RAM++ uses **per-class thresholds** rather than a global threshold [HIGH]:

- Default threshold: 0.68
- Individual thresholds loaded from `ram_tag_list_threshold.txt`
- Thresholds calibrated based on class frequency and difficulty

---

## Training Data & Methodology

### Training Datasets

RAM++ is trained on large-scale image-text pairs [1][HIGH]:

| Dataset | Size | Type |
|---------|------|------|
| COCO | 118K images | Captions + annotations |
| Visual Genome (VG) | 108K images | Dense annotations |
| SBU Captions | 1M images | Web captions |
| CC3M | 3M images | Web image-text pairs |
| CC3M-val | Validation split | Quality filtered |
| CC12M | 12M images | Web image-text pairs |

**Total**: ~14 million image-text pairs (hence "ram_plus_swin_large_14m")

### Four-Phase Training Pipeline

1. **Automatic Text Parsing**: Generate annotation-free tags from captions at scale
2. **Preliminary Training**: Unify caption and tagging tasks
3. **Data Engine**: Generate and clean annotations automatically
4. **Fine-tuning**: Retrain with processed data + higher-quality datasets

---

## Benchmark Performance

### Comparison with CLIP

| Benchmark | Metric | CLIP | RAM++ | Improvement |
|-----------|--------|------|-------|-------------|
| OpenImages (common) | mAP | - | - | +10.2 |
| ImageNet (common) | mAP | - | - | +15.4 |
| OpenImages (open-set) | mAP | - | - | +5.0 vs CLIP |
| OpenImages (open-set) | mAP | - | - | +6.4 vs RAM |
| HICO (HOI phrases) | mAP | - | - | +7.8 vs CLIP |
| HICO (HOI phrases) | mAP | - | - | +4.7 vs RAM |

[1][HIGH]

### Inference Performance

Benchmarked on 144,485 images across 167 nodes with 19 GPU types [4][HIGH]:

| Metric | Value |
|--------|-------|
| Average Inference Time | <300ms |
| Best Cost Efficiency | 309K images/$ (RTX 2080) |
| Model Size | 3GB |
| Memory Requirement | 8GB RAM minimum |

### GPU Performance Distribution

| GPU | Performance |
|-----|-------------|
| RTX 2080 | Best cost efficiency |
| RTX 3080/3090 | Fast inference, higher cost |
| RTX 20/30 series | Generally optimal |

Note: Wide inference time variation observed within single GPU types [4][MEDIUM].

---

## Installation & Dependencies

### Python Package Installation

```bash
pip install git+https://github.com/xinyu1205/recognize-anything.git
```

### Required Dependencies

| Package | Version | Purpose |
|---------|---------|---------|
| torch | ≥2.0 | Core ML framework |
| torchvision | ≥0.15 | Image processing |
| timm | ≥0.9 | Vision model components |
| scipy | ≥1.10.0 | Interpolation functions |
| fairscale | ≥0.4.0 | Gradient checkpointing |
| einops | ≥0.7.0 | Tensor operations |
| transformers | ≥4.40.0 | BERT tokenizer |

**Note**: The RAM package does not properly declare all dependencies. The above list was determined empirically [HIGH].

### Model Weights

Download from Hugging Face:
- **URL**: https://huggingface.co/xinyu1205/recognize-anything-plus-model
- **File**: `ram_plus_swin_large_14m.pth` (~2.9GB)

---

## Usage Examples

### Basic Inference (CLI)

```bash
python inference_ram_plus.py \
  --image images/demo/demo1.jpg \
  --pretrained pretrained/ram_plus_swin_large_14m.pth
```

### Python API

```python
import torch
from PIL import Image
from ram import get_transform, inference_ram
from ram.models import ram_plus

# Load model
model = ram_plus(
    pretrained="path/to/ram_plus_swin_large_14m.pth",
    image_size=384,
    vit="swin_l"
)
model.eval()
model = model.to("cuda")

# Prepare image
transform = get_transform(image_size=384)
image = Image.open("image.jpg").convert("RGB")
image_tensor = transform(image).unsqueeze(0).to("cuda")

# Run inference
with torch.no_grad():
    tags, _ = inference_ram(image_tensor, model)

# Parse results (pipe-separated)
tag_list = [t.strip() for t in tags.split("|")]
print(tag_list)
```

### Open-Set Recognition

For recognizing categories beyond the predefined vocabulary:

```bash
python inference_ram_plus_openset.py \
  --image images/openset_example.jpg \
  --pretrained pretrained/ram_plus_swin_large_14m.pth \
  --llm_tag_des datasets/openimages_rare_200/openimages_rare_200_llm_tag_descriptions.json
```

---

## Integration with Other Models

### Grounded-SAM Pipeline

RAM++ can be combined with SAM for visual semantic analysis [2]:

```
Image → RAM++ (Recognition) → Grounded-SAM (Localization) → Segmented Objects with Labels
```

### Visual Buffet Integration

In the visual-buffet project, RAM++ is implemented as a plugin:

```python
from visual_buffet.plugins import RamPlusPlugin

plugin = RamPlusPlugin(plugin_dir)
result = plugin.tag(image_path)

for tag in result.tags:
    print(f"- {tag.label}")  # No confidence scores
```

**Note**: RAM++ returns tags without confidence scores, unlike SigLIP which provides per-tag probabilities.

---

## Comparison with Alternatives

| Model | Tags | Open-Set | Confidence | Speed | Size |
|-------|------|----------|------------|-------|------|
| **RAM++** | 4,585 | Yes | Yes* | ~200ms | 3GB |
| RAM | 6,500+ | Limited | No | ~200ms | 2.8GB |
| CLIP | Unlimited | Yes | Yes | ~50ms | 400MB-2GB |
| BLIP | Unlimited | Yes | Yes | ~100ms | 1-2GB |
| SigLIP | Custom | Yes | Yes | ~500ms | 3.5GB |

*RAM++ confidence: Extracted from internal sigmoid(logits) by Visual Buffet. Not exposed by default inference API.

### When to Use RAM++

**Choose RAM++ when:**
- You need broad vocabulary coverage (4,585 categories)
- Open-set recognition of uncommon categories is important
- You don't need per-tag confidence scores
- Cost-efficient batch processing is priority

**Choose alternatives when:**
- You need confidence scores (use SigLIP or CLIP)
- Custom vocabulary is required (use CLIP/SigLIP zero-shot)
- Minimal latency is critical (use CLIP)

---

## Limitations & Uncertainties

### What This Document Does NOT Cover

- Fine-tuning procedures for custom datasets
- Training from scratch methodology
- Detailed ablation study results
- RAM++ variants beyond swin_large_14m

### Known Limitations

1. **~~No Confidence Scores~~**: RESOLVED in Visual Buffet v1.0 - now extracts sigmoid probabilities from model internals [HIGH]
2. **Fixed Vocabulary**: While open-set capable, best performance is on predefined 4,585 tags [MEDIUM]
3. **Inference Variability**: Wide distribution in inference times even within same GPU type [4][MEDIUM]
4. **Undeclared Dependencies**: Package doesn't declare scipy, fairscale, einops as requirements [HIGH]

### Unverified Claims

- Exact mAP numbers on benchmarks (only deltas vs CLIP reported in abstract) [MEDIUM]
- Performance on non-English content [LOW]

### Source Conflicts

None identified - sources are consistent.

### Knowledge Gaps

- Detailed breakdown of the 4,585 tag categories by type
- Memory usage during inference by GPU type
- Performance on video frames vs static images

---

## Recommendations

1. **For production deployment**: Use RTX 20/30 series GPUs for optimal cost-efficiency
2. **For dependency management**: Explicitly install scipy, fairscale, einops alongside ram package
3. **For confidence scores**: Pair RAM++ with SigLIP for complementary tag + confidence output
4. **For custom vocabularies**: Consider CLIP or SigLIP zero-shot instead
5. **For batch processing**: RAM++ excels at high-throughput scenarios (309K images/$)

---

## Source Appendix

| # | Source | Date | Type | Used For |
|---|--------|------|------|----------|
| 1 | [arXiv:2310.15200 - Open-Set Image Tagging with Multi-Grained Text Supervision](https://arxiv.org/abs/2310.15200) | Oct 2023 | Primary (Paper) | Architecture, benchmarks, methodology |
| 2 | [GitHub - xinyu1205/recognize-anything](https://github.com/xinyu1205/recognize-anything) | 2023-2024 | Primary (Code) | Installation, usage, API |
| 3 | [arXiv:2306.03514 - Recognize Anything: A Strong Image Tagging Model](https://arxiv.org/abs/2306.03514) | Jun 2023 | Primary (Paper) | RAM background, training approach |
| 4 | [SaladCloud - Tag 309K Images/$ with RAM++](https://blog.salad.com/recognize-anything-model/) | 2024 | Secondary | Performance benchmarks, deployment |
| 5 | [Hugging Face - recognize-anything-plus-model](https://huggingface.co/xinyu1205/recognize-anything-plus-model) | 2023-2024 | Primary | Model weights, metadata |
| 6 | RAM++ source code analysis | Dec 2025 | Primary (Code) | Architecture details, dependencies |

---

## Document History

| Version | Date | Changes |
|---------|------|---------|
| 1.0 | 2025-12-21 | Initial version |
